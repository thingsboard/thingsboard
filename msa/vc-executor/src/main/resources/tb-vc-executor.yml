#
# Copyright © 2016-2026 The Thingsboard Authors
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Spring common parameters
spring.main.web-environment: "${WEB_APPLICATION_ENABLE:false}" # If you enabled process metrics you should also enable 'web-environment'.
spring.main.web-application-type: "${WEB_APPLICATION_TYPE:none}" # If you enabled process metrics you should set 'web-application-type' to 'servlet' value.
spring.main.allow-circular-references: "true" # Spring Boot configuration property that controls whether circular dependencies between beans are allowed.

# Server common parameters
server:
  # Server bind address (has no effect if web-environment is disabled).
  address: "${HTTP_BIND_ADDRESS:0.0.0.0}"
  # Server bind port (has no effect if web-environment is disabled).
  port: "${HTTP_BIND_PORT:8086}"

# Zookeeper connection parameters. Used for service discovery.
zk:
  # Enable/disable zookeeper discovery service.
  enabled: "${ZOOKEEPER_ENABLED:true}"
  # Zookeeper connect string
  url: "${ZOOKEEPER_URL:localhost:2181}"
  # Zookeeper retry interval in milliseconds
  retry_interval_ms: "${ZOOKEEPER_RETRY_INTERVAL_MS:3000}"
  # Zookeeper connection timeout in milliseconds
  connection_timeout_ms: "${ZOOKEEPER_CONNECTION_TIMEOUT_MS:3000}"
  # Zookeeper session timeout in milliseconds
  session_timeout_ms: "${ZOOKEEPER_SESSION_TIMEOUT_MS:3000}"
  # Name of the directory in zookeeper 'filesystem'
  zk_dir: "${ZOOKEEPER_NODES_DIR:/thingsboard}"
  # The recalculate_delay property is recommended in a microservices architecture setup for rule-engine services.
  # This property provides a pause to ensure that when a rule-engine service is restarted, other nodes don't immediately attempt to recalculate their partitions.
  # The delay is recommended because the initialization of rule chain actors is time-consuming. Avoiding unnecessary recalculations during a restart can enhance system performance and stability.
  recalculate_delay: "${ZOOKEEPER_RECALCULATE_DELAY_MS:0}"

# Queue configuration parameters
queue:
  type: "${TB_QUEUE_TYPE:kafka}" # kafka (Apache Kafka)
  prefix: "${TB_QUEUE_PREFIX:}" # Global queue prefix. If specified, prefix is added before default topic name: 'prefix.default_topic_name'. Prefix is applied to all topics (and consumer groups for kafka).
  in_memory:
    stats:
      # For debug lvl
      print-interval-ms: "${TB_QUEUE_IN_MEMORY_STATS_PRINT_INTERVAL_MS:60000}"
  kafka:
    # Kafka Bootstrap Servers
    bootstrap.servers: "${TB_KAFKA_SERVERS:localhost:9092}"
    ssl:
      # Enable/Disable SSL Kafka communication
      enabled: "${TB_KAFKA_SSL_ENABLED:false}"
      # The location of the trust store file
      truststore.location: "${TB_KAFKA_SSL_TRUSTSTORE_LOCATION:}"
      # The password of trust store file if specified
      truststore.password: "${TB_KAFKA_SSL_TRUSTSTORE_PASSWORD:}"
      # The location of the key store file. This is optional for the client and can be used for two-way authentication for the client
      keystore.location: "${TB_KAFKA_SSL_KEYSTORE_LOCATION:}"
      # The store password for the key store file. This is optional for the client and only needed if ‘ssl.keystore.location’ is configured. Key store password is not supported for PEM format
      keystore.password: "${TB_KAFKA_SSL_KEYSTORE_PASSWORD:}"
      # The password of the private key in the key store file or the PEM key specified in ‘keystore.key’
      key.password: "${TB_KAFKA_SSL_KEY_PASSWORD:}"
    # The number of acknowledgments the producer requires the leader to have received before considering a request complete. This controls the durability of records that are sent. The following settings are allowed:0,1 and all
    acks: "${TB_KAFKA_ACKS:all}"
    # Number of retries. Resend any record whose send fails with a potentially transient error
    retries: "${TB_KAFKA_RETRIES:1}"
    compression.type: "${TB_KAFKA_COMPRESSION_TYPE:none}" # none or gzip
    # Default batch size. This setting gives the upper bound of the batch size to be sent
    batch.size: "${TB_KAFKA_BATCH_SIZE:16384}"
    # This variable creates a small amount of artificial delay—that is, rather than immediately sending out a record
    linger.ms: "${TB_KAFKA_LINGER_MS:1}"
    # The maximum size of a request in bytes. This setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests
    max.request.size: "${TB_KAFKA_MAX_REQUEST_SIZE:1048576}"
    # The maximum number of unacknowledged requests the client will send on a single connection before blocking
    max.in.flight.requests.per.connection: "${TB_KAFKA_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION:5}"
    # The total bytes of memory the producer can use to buffer records waiting to be sent to the server
    buffer.memory: "${TB_BUFFER_MEMORY:33554432}"
    # The multiple copies of data over the multiple brokers of Kafka
    replication_factor: "${TB_QUEUE_KAFKA_REPLICATION_FACTOR:1}"
    # The maximum delay between invocations of poll() when using consumer group management. This places an upper bound on the amount of time that the consumer can be idle before fetching more records
    max_poll_interval_ms: "${TB_QUEUE_KAFKA_MAX_POLL_INTERVAL_MS:300000}"
    # The maximum number of records returned in a single call to poll()
    max_poll_records: "${TB_QUEUE_KAFKA_MAX_POLL_RECORDS:8192}"
    # The maximum amount of data per-partition the server will return. Records are fetched in batches by the consumer
    max_partition_fetch_bytes: "${TB_QUEUE_KAFKA_MAX_PARTITION_FETCH_BYTES:16777216}"
    # The maximum amount of data the server will return. Records are fetched in batches by the consumer
    fetch_max_bytes: "${TB_QUEUE_KAFKA_FETCH_MAX_BYTES:134217728}"
    request.timeout.ms: "${TB_QUEUE_KAFKA_REQUEST_TIMEOUT_MS:30000}" # (30 seconds) # refer to https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#producerconfigs_request.timeout.ms
    session.timeout.ms: "${TB_QUEUE_KAFKA_SESSION_TIMEOUT_MS:10000}" # (10 seconds) # refer to https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#consumerconfigs_session.timeout.ms
    auto_offset_reset: "${TB_QUEUE_KAFKA_AUTO_OFFSET_RESET:earliest}" # earliest, latest or none
    # Enable/Disable using of Confluent Cloud
    use_confluent_cloud: "${TB_QUEUE_KAFKA_USE_CONFLUENT_CLOUD:false}"
    confluent:
      # The endpoint identification algorithm used by clients to validate server hostname. The default value is https
      ssl.algorithm: "${TB_QUEUE_KAFKA_CONFLUENT_SSL_ALGORITHM:https}"
      # The mechanism used to authenticate Schema Registry requests. SASL/PLAIN should only be used with TLS/SSL as a transport layer to ensure that clear passwords are not transmitted on the wire without encryption
      sasl.mechanism: "${TB_QUEUE_KAFKA_CONFLUENT_SASL_MECHANISM:PLAIN}"
      # Using JAAS Configuration for specifying multiple SASL mechanisms on a broker
      sasl.config: "${TB_QUEUE_KAFKA_CONFLUENT_SASL_JAAS_CONFIG:org.apache.kafka.common.security.plain.PlainLoginModule required username=\"CLUSTER_API_KEY\" password=\"CLUSTER_API_SECRET\";}"
      # Protocol used to communicate with brokers. Valid values are: PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL
      security.protocol: "${TB_QUEUE_KAFKA_CONFLUENT_SECURITY_PROTOCOL:SASL_SSL}"
    # Key-value properties for Kafka consumer per specific topic, e.g. tb_ota_package is a topic name for ota, tb_rule_engine.sq is a topic name for default SequentialByOriginator queue.
    # Check TB_QUEUE_CORE_OTA_TOPIC and TB_QUEUE_RE_SQ_TOPIC params
    consumer-properties-per-topic:
      tb_ota_package:
        # Key-value properties for Kafka consumer per specific topic, e.g. tb_ota_package is a topic name for ota, tb_rule_engine.sq is a topic name for default SequentialByOriginator queue. Check TB_QUEUE_CORE_OTA_TOPIC and TB_QUEUE_RE_SQ_TOPIC params
        - key: max.poll.records
          # Example of specific consumer properties value per topic
          value: "${TB_QUEUE_KAFKA_OTA_MAX_POLL_RECORDS:10}"
      tb_version_control:
        # Example of specific consumer properties value per topic for VC
        - key: max.poll.interval.ms
          # Example of specific consumer properties value per topic for VC
          value: "${TB_QUEUE_KAFKA_VC_MAX_POLL_INTERVAL_MS:600000}"
    #      tb_rule_engine.sq:
    #        - key: max.poll.records
    #          value: "${TB_QUEUE_KAFKA_SQ_MAX_POLL_RECORDS:1024}"
    # If you override any default Kafka topic name using environment variables, you must also specify the related consumer properties
    # for the new topic in `consumer-properties-per-topic-inline`. Otherwise, the topic will not inherit its expected configuration (e.g., max.poll.records, timeouts, etc).
    # Format: "topic1:key1=value1,key2=value2;topic2:key=value"
    # Example: "tb_core_modified.notifications:max.poll.records=10;tb_edge_modified:max.poll.records=10,enable.auto.commit=true"
    consumer-properties-per-topic-inline: "${TB_QUEUE_KAFKA_CONSUMER_PROPERTIES_PER_TOPIC_INLINE:}"
    other-inline: "${TB_QUEUE_KAFKA_OTHER_PROPERTIES:}" # In this section you can specify custom parameters (semicolon separated) for Kafka consumer/producer/admin # Example "metrics.recording.level:INFO;metrics.sample.window.ms:30000"
    other: # DEPRECATED. In this section you can specify custom parameters for Kafka consumer/producer and expose the env variables to configure outside
    #  - key: "request.timeout.ms" # refer to https://docs.confluent.io/platform/current/installation/configuration/producer-configs.html#producerconfigs_request.timeout.ms
    #    value: "${TB_QUEUE_KAFKA_REQUEST_TIMEOUT_MS:30000}" # (30 seconds)
    #  - key: "session.timeout.ms" # refer to https://docs.confluent.io/platform/current/installation/configuration/consumer-configs.html#consumerconfigs_session.timeout.ms
    #    value: "${TB_QUEUE_KAFKA_SESSION_TIMEOUT_MS:10000}" # (10 seconds)
    topic-properties:
      # Kafka properties for Core topics
      core: "${TB_QUEUE_KAFKA_CORE_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for Notifications topics
      notifications: "${TB_QUEUE_KAFKA_NOTIFICATIONS_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for Core topics
      version-control: "${TB_QUEUE_KAFKA_VC_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:1;min.insync.replicas:1}"
      # Kafka properties for Housekeeper tasks topic
      housekeeper: "${TB_QUEUE_KAFKA_HOUSEKEEPER_TOPIC_PROPERTIES:retention.ms:604800000;segment.bytes:52428800;retention.bytes:1048576000;partitions:10;min.insync.replicas:1}"
    consumer-stats:
      # Prints lag between consumer group offset and last messages offset in Kafka topics
      enabled: "${TB_QUEUE_KAFKA_CONSUMER_STATS_ENABLED:true}"
      # Statistics printing interval for Kafka's consumer-groups stats
      print-interval-ms: "${TB_QUEUE_KAFKA_CONSUMER_STATS_MIN_PRINT_INTERVAL_MS:60000}"
      # Time to wait for the stats-loading requests to Kafka to finis
      kafka-response-timeout-ms: "${TB_QUEUE_KAFKA_CONSUMER_STATS_RESPONSE_TIMEOUT_MS:1000}"
    # Topics cache TTL in milliseconds. 5 minutes by default
    topics_cache_ttl_ms: "${TB_QUEUE_KAFKA_TOPICS_CACHE_TTL_MS:300000}"
  partitions:
    hash_function_name: "${TB_QUEUE_PARTITIONS_HASH_FUNCTION_NAME:murmur3_128}" # murmur3_32, murmur3_128 or sha256
  core:
    # Default topic name
    topic: "${TB_QUEUE_CORE_TOPIC:tb_core}"
    # For high-priority notifications that require minimum latency and processing time
    notifications_topic: "${TB_QUEUE_CORE_NOTIFICATIONS_TOPIC:tb_core.notifications}"
    # Interval in milliseconds to poll messages by Core microservices
    poll-interval: "${TB_QUEUE_CORE_POLL_INTERVAL_MS:25}"
    # Amount of partitions used by Core microservices
    partitions: "${TB_QUEUE_CORE_PARTITIONS:10}"
    # Timeout for processing a message pack by Core microservices
    pack-processing-timeout: "${TB_QUEUE_CORE_PACK_PROCESSING_TIMEOUT_MS:2000}"
    ota:
      # Default topic name for OTA updates
      topic: "${TB_QUEUE_CORE_OTA_TOPIC:tb_ota_package}"
      # The interval of processing the OTA updates for devices. Used to avoid any harm to the network due to many parallel OTA updates
      pack-interval-ms: "${TB_QUEUE_CORE_OTA_PACK_INTERVAL_MS:60000}"
      # The size of OTA updates notifications fetched from the queue. The queue stores pairs of firmware and device ids
      pack-size: "${TB_QUEUE_CORE_OTA_PACK_SIZE:100}"
    # Stats topic name
    usage-stats-topic: "${TB_QUEUE_US_TOPIC:tb_usage_stats}"
    stats:
      # Enable/disable statistics for Core microservices
      enabled: "${TB_QUEUE_CORE_STATS_ENABLED:true}"
      # Statistics printing interval for Core microservices
      print-interval-ms: "${TB_QUEUE_CORE_STATS_PRINT_INTERVAL_MS:60000}"
    housekeeper:
      # Topic name for Housekeeper tasks
      topic: "${TB_HOUSEKEEPER_TOPIC:tb_housekeeper}"
  vc:
    # Default topic name
    topic: "${TB_QUEUE_VC_TOPIC:tb_version_control}"
    # Number of partitions to associate with this queue. Used for scaling the number of messages that can be processed in parallel
    partitions: "${TB_QUEUE_VC_PARTITIONS:10}"
    # Interval in milliseconds between polling of the messages if no new messages arrive
    poll-interval: "${TB_QUEUE_VC_INTERVAL_MS:25}"
    # Timeout before retrying all failed and timed-out messages from the processing pack
    pack-processing-timeout: "${TB_QUEUE_VC_PACK_PROCESSING_TIMEOUT_MS:180000}"
    # Limit for single queue message size
    msg-chunk-size: "${TB_QUEUE_VC_MSG_CHUNK_SIZE:250000}"

# Version control parameters
vc:
  # Pool size for handling export tasks
  thread_pool_size: "${TB_VC_POOL_SIZE:6}"
  git:
    # Pool size for handling the git IO operations
    io_pool_size: "${TB_VC_GIT_POOL_SIZE:3}"
    # Default storing repository path
    repositories-folder: "${TB_VC_GIT_REPOSITORIES_FOLDER:${java.io.tmpdir}/repositories}"

# Usage statistics parameters
usage:
  stats:
    report:
      # Enable/Disable the collection of statistics about API usage. Collected on a system and tenant level by default
      enabled: "${USAGE_STATS_REPORT_ENABLED:true}"
      # Enable/Disable collection of statistics about API usage on a customer level
      enabled_per_customer: "${USAGE_STATS_REPORT_PER_CUSTOMER_ENABLED:false}"
      # Interval of reporting the statistics. By default, the summarized statistics are sent every 10 seconds
      interval: "${USAGE_STATS_REPORT_INTERVAL:60}"
      # Amount of statistic messages in pack
      pack_size: "${USAGE_STATS_REPORT_PACK_SIZE:1024}"

# Metrics parameters
metrics:
  # Enable/disable actuator metrics.
  enabled: "${METRICS_ENABLED:false}"
  timer:
    # Metrics percentiles returned by actuator for timer metrics. List of double values (divided by ,).
    percentiles: "${METRICS_TIMER_PERCENTILES:0.5}"

# General management parameters
management:
  endpoints:
    web:
      exposure:
        # Expose metrics endpoint (use value 'prometheus' to enable prometheus metrics).
        include: "${METRICS_ENDPOINTS_EXPOSE:info}"

# Service common properties
service:
  type: "${TB_SERVICE_TYPE:tb-vc-executor}" # service type
  # Unique id for this service (autogenerated if empty)
  id: "${TB_SERVICE_ID:}"

# Notification system parameters
notification_system:
  rules:
    # Semicolon-separated deduplication durations (in millis) for trigger types. Format: 'NotificationRuleTriggerType1:123;NotificationRuleTriggerType2:456'
    deduplication_durations: "${TB_NOTIFICATION_RULES_DEDUPLICATION_DURATIONS:RATE_LIMITS:14400000;}"
